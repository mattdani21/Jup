{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIwtj/rZRUdvecKCdd14xE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattdani21/Jup/blob/main/Final_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code cell is the initial setup for your Spark-based text classification project.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "*   **Comments:** The initial comments describe the purpose of the notebook: building an improved multi-class text classification model using word embeddings and addressing issues from a previous version.\n",
        "*   **Import Statements:** This section imports all the necessary libraries and modules for the project:\n",
        "    *   `pyspark.sql`: For working with Spark DataFrames and SQL functions.\n",
        "    *   `pyspark.ml.feature`: For feature engineering transformers like `StopWordsRemover` and `Word2Vec`.\n",
        "    *   `pyspark.ml.classification`: For classification algorithms like `LogisticRegression` and `RandomForestClassifier`.\n",
        "    *   `pyspark.ml.evaluation`: For evaluating the performance of the classification models (`MulticlassClassificationEvaluator`).\n",
        "    *   `pyspark.ml.linalg`: For working with Spark's distributed linear algebra (`Vectors`, `VectorUDT`).\n",
        "    *   `pyspark.sql.functions.udf`: To define User Defined Functions (UDFs) for custom transformations.\n",
        "    *   `numpy`: For numerical operations, particularly for handling word vectors.\n",
        "    *   `sklearn.datasets.fetch_20newsgroups`: To fetch the 20newsgroups dataset, a common dataset for text classification.\n",
        "    *   `pandas`: For initial data handling and manipulation before converting to a Spark DataFrame.\n",
        "    *   `matplotlib.pyplot` and `seaborn`: Although imported, they are not currently used in the visible code. They are typically used for data visualization.\n",
        "*   **Spark Initialization:** The line `spark = SparkSession.builder.master(\"local[*]\").appName(\"ImprovedClassification\").getOrCreate()` initializes a Spark session.\n",
        "    *   `.master(\"local[*]\")`: Configures Spark to run locally using all available cores.\n",
        "    *   `.appName(\"ImprovedClassification\")`: Sets a name for the Spark application.\n",
        "    *   `.getOrCreate()`: Gets an existing Spark session or creates a new one if none exists.\n",
        "\n",
        "In essence, this cell prepares the environment by importing the required tools and starting a Spark session, making it ready for the subsequent data loading, preprocessing, model training, and evaluation steps.\n"
      ],
      "metadata": {
        "id": "rrfd5BAyjU5N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9S0XPG6D3cPf"
      },
      "outputs": [],
      "source": [
        "# Improved Multi-Class Text Classification with Word Embeddings\n",
        "# This enhanced version addresses key issues in the original model\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lower, regexp_replace, split, col, size, when, isnan, isnull\n",
        "from pyspark.ml.feature import StopWordsRemover, Word2Vec, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.sql.functions import udf\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"ImprovedClassification\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 1: IMPROVED DATA PREPROCESSING\n",
        "def load_and_preprocess_data():\n",
        "    \"\"\"Load and preprocess the 20newsgroups dataset with better cleaning\"\"\"\n",
        "\n",
        "    # Fetch dataset with better category selection (fewer, more distinct categories)\n",
        "    categories = [\n",
        "        'alt.atheism',\n",
        "        'comp.graphics',\n",
        "        'rec.motorcycles',\n",
        "        'sci.space',\n",
        "        'talk.politics.guns'\n",
        "    ]\n",
        "\n",
        "    newsgroups = fetch_20newsgroups(\n",
        "        subset='all',\n",
        "        categories=categories,\n",
        "        remove=('headers', 'footers', 'quotes'),\n",
        "        shuffle=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Create pandas DataFrame\n",
        "    df_pandas = pd.DataFrame({\n",
        "        'text': newsgroups.data,\n",
        "        'label': newsgroups.target\n",
        "    })\n",
        "\n",
        "    # Filter out very short documents (less than 10 words)\n",
        "    df_pandas = df_pandas[df_pandas['text'].str.split().str.len() >= 10]\n",
        "\n",
        "    # Convert to Spark DataFrame\n",
        "    df_labeled = spark.createDataFrame(df_pandas)\n",
        "\n",
        "    print(f\"Dataset loaded with {df_labeled.count()} documents\")\n",
        "    print(\"Target classes:\")\n",
        "    for i, name in enumerate(newsgroups.target_names):\n",
        "        print(f\"{i}: {name}\")\n",
        "\n",
        "    return df_labeled, newsgroups.target_names\n",
        "\n",
        "def advanced_text_preprocessing(df):\n",
        "    \"\"\"Enhanced text preprocessing with better cleaning\"\"\"\n",
        "\n",
        "    # More comprehensive text cleaning\n",
        "    df_clean = df.select(\n",
        "        'label',\n",
        "        # Remove URLs, email addresses, numbers, and special characters\n",
        "        regexp_replace(\n",
        "            regexp_replace(\n",
        "                regexp_replace(\n",
        "                    regexp_replace('text', r'http\\S+|www\\S+', ''),  # URLs\n",
        "                    r'\\S+@\\S+', ''  # Email addresses\n",
        "                ),\n",
        "                r'\\d+', ''  # Numbers\n",
        "            ),\n",
        "            r'[^\\w\\s]', ' '  # Special characters\n",
        "        ).alias('cleaned_text')\n",
        "    )\n",
        "\n",
        "    # Tokenize and convert to lowercase\n",
        "    df_tokens = df_clean.select(\n",
        "        'label',\n",
        "        split(lower(col('cleaned_text')), r'\\s+').alias('tokens')\n",
        "    )\n",
        "\n",
        "    # Filter out empty tokens and tokens with length < 3\n",
        "    df_filtered = df_tokens.select(\n",
        "        'label',\n",
        "        col('tokens').alias('raw_tokens')\n",
        "    )\n",
        "\n",
        "    # Remove stop words\n",
        "    remover = StopWordsRemover(inputCol=\"raw_tokens\", outputCol=\"tokens\")\n",
        "    df_no_stopwords = remover.transform(df_filtered)\n",
        "\n",
        "    # Filter out documents with too few tokens after cleaning\n",
        "    df_final = df_no_stopwords.filter(size(col('tokens')) >= 5)\n",
        "\n",
        "    print(f\"After preprocessing: {df_final.count()} documents remain\")\n",
        "    return df_final.select('label', 'tokens')\n",
        "\n",
        "print('ready')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9NwyC1u3zRW",
        "outputId": "04922c82-2de1-42e3-f0ab-2b91e2e609df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: IMPROVED WORD2VEC TRAINING\n",
        "\n",
        "def train_improved_word2vec(df_processed):\n",
        "    \"\"\"Train Word2Vec with better parameters\"\"\"\n",
        "\n",
        "    # Use better parameters for Word2Vec\n",
        "    word2vec = Word2Vec(\n",
        "        vectorSize=200,        # Increased vector size\n",
        "        minCount=3,           # Lower minimum count to capture more words\n",
        "        numPartitions=4,      # Better parallelization\n",
        "        stepSize=0.05,        # Learning rate\n",
        "        maxIter=5,            # More iterations\n",
        "        windowSize=7,         # Larger context window\n",
        "        inputCol=\"tokens\",\n",
        "        outputCol=\"word_vectors\"\n",
        "    )\n",
        "\n",
        "    print(\"Training Word2Vec model...\")\n",
        "    model = word2vec.fit(df_processed)\n",
        "\n",
        "    # Get vocabulary statistics\n",
        "    vocab_size = model.getVectors().count()\n",
        "    print(f\"Word2Vec vocabulary size: {vocab_size}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "wSJhsZXv4OKe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 3: IMPROVED DOCUMENT VECTORIZATION\n",
        "\n",
        "def create_document_vectors_improved(df_processed, word2vec_model):\n",
        "    \"\"\"Create document vectors with TF-IDF weighting and better handling\"\"\"\n",
        "\n",
        "    # Get word vectors\n",
        "    word_vectors = word2vec_model.getVectors()\n",
        "    word_vectors_dict = {row['word']: row['vector'] for row in word_vectors.collect()}\n",
        "    word_vectors_broadcast = spark.sparkContext.broadcast(word_vectors_dict)\n",
        "\n",
        "    # Calculate TF (Term Frequency) for each document\n",
        "    def calculate_tf_idf_weighted_average(tokens):\n",
        "        \"\"\"Calculate TF-IDF weighted average of word vectors\"\"\"\n",
        "        if not tokens:\n",
        "            return Vectors.dense([0.0] * 200)\n",
        "\n",
        "        # Count word frequencies in document\n",
        "        word_counts = {}\n",
        "        for word in tokens:\n",
        "            word_counts[word] = word_counts.get(word, 0) + 1\n",
        "\n",
        "        doc_length = len(tokens)\n",
        "        vectors = []\n",
        "        weights = []\n",
        "\n",
        "        for word, count in word_counts.items():\n",
        "            if word in word_vectors_broadcast.value:\n",
        "                tf = count / doc_length  # Term frequency\n",
        "                vectors.append(word_vectors_broadcast.value[word])\n",
        "                weights.append(tf)\n",
        "\n",
        "        if not vectors:\n",
        "            return Vectors.dense([0.0] * 200)\n",
        "\n",
        "        # Weighted average\n",
        "        vectors = np.array(vectors)\n",
        "        weights = np.array(weights)\n",
        "        weights = weights / np.sum(weights)  # Normalize weights\n",
        "\n",
        "        weighted_avg = np.average(vectors, axis=0, weights=weights)\n",
        "        return Vectors.dense(weighted_avg)\n",
        "\n",
        "    # Register UDF\n",
        "    vectorize_udf = udf(calculate_tf_idf_weighted_average, VectorUDT())\n",
        "\n",
        "    # Apply vectorization\n",
        "    df_vectors = df_processed.withColumn(\"features\", vectorize_udf(col(\"tokens\")))\n",
        "\n",
        "    # Filter out documents with zero vectors\n",
        "    df_vectors = df_vectors.filter(\n",
        "        ~(col(\"features\").isNull())\n",
        "    )\n",
        "\n",
        "    print(f\"Document vectors created for {df_vectors.count()} documents\")\n",
        "    return df_vectors.select('label', 'features')\n"
      ],
      "metadata": {
        "id": "KlMZp1QC4kAj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 4: IMPROVED CLASSIFICATION\n",
        "# ============================================================================\n",
        "\n",
        "def train_improved_classifier(df_features):\n",
        "    \"\"\"Train multiple classifiers and compare performance\"\"\"\n",
        "\n",
        "    # Split data with stratification consideration\n",
        "    train_data, test_data = df_features.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "    print(f\"Training set size: {train_data.count()}\")\n",
        "    print(f\"Test set size: {test_data.count()}\")\n",
        "\n",
        "    # Train Logistic Regression with better parameters\n",
        "    lr = LogisticRegression(\n",
        "        featuresCol='features',\n",
        "        labelCol='label',\n",
        "        maxIter=100,           # More iterations\n",
        "        regParam=0.01,         # L2 regularization\n",
        "        elasticNetParam=0.1,   # Some L1 regularization\n",
        "        standardization=True   # Feature standardization\n",
        "    )\n",
        "\n",
        "    print(\"Training Logistic Regression...\")\n",
        "    lr_model = lr.fit(train_data)\n",
        "\n",
        "    # Train Random Forest as alternative\n",
        "    rf = RandomForestClassifier(\n",
        "        featuresCol='features',\n",
        "        labelCol='label',\n",
        "        numTrees=50,          # More trees\n",
        "        maxDepth=10,          # Deeper trees\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    print(\"Training Random Forest...\")\n",
        "    rf_model = rf.fit(train_data)\n",
        "\n",
        "    return lr_model, rf_model, train_data, test_data\n",
        "\n",
        "def evaluate_models(models, test_data, target_names):\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "\n",
        "    evaluator_acc = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"label\",\n",
        "        predictionCol=\"prediction\",\n",
        "        metricName=\"accuracy\"\n",
        "    )\n",
        "\n",
        "    evaluator_f1 = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"label\",\n",
        "        predictionCol=\"prediction\",\n",
        "        metricName=\"f1\"\n",
        "    )\n",
        "\n",
        "    evaluator_precision = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"label\",\n",
        "        predictionCol=\"prediction\",\n",
        "        metricName=\"weightedPrecision\"\n",
        "    )\n",
        "\n",
        "    evaluator_recall = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"label\",\n",
        "        predictionCol=\"prediction\",\n",
        "        metricName=\"weightedRecall\"\n",
        "    )\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\n=== {name} Results ===\")\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = model.transform(test_data)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = evaluator_acc.evaluate(predictions)\n",
        "        f1 = evaluator_f1.evaluate(predictions)\n",
        "        precision = evaluator_precision.evaluate(predictions)\n",
        "        recall = evaluator_recall.evaluate(predictions)\n",
        "\n",
        "        results[name] = {\n",
        "            'accuracy': accuracy,\n",
        "            'f1': f1,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'predictions': predictions\n",
        "        }\n",
        "\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"F1 Score: {f1:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "        # Show some example predictions\n",
        "        print(\"\\nSample Predictions:\")\n",
        "        predictions.select('label', 'prediction').show(10)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "0z363xj24mzj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 5: MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    print(\"=== IMPROVED TEXT CLASSIFICATION MODEL ===\\n\")\n",
        "\n",
        "    # Load and preprocess data\n",
        "    print(\"1. Loading and preprocessing data...\")\n",
        "    df_labeled, target_names = load_and_preprocess_data()\n",
        "    df_processed = advanced_text_preprocessing(df_labeled)\n",
        "\n",
        "    # Train Word2Vec\n",
        "    print(\"\\n2. Training improved Word2Vec model...\")\n",
        "    word2vec_model = train_improved_word2vec(df_processed)\n",
        "\n",
        "    # Create document vectors\n",
        "    print(\"\\n3. Creating document vectors...\")\n",
        "    df_features = create_document_vectors_improved(df_processed, word2vec_model)\n",
        "\n",
        "    # Train classifiers\n",
        "    print(\"\\n4. Training classifiers...\")\n",
        "    lr_model, rf_model, train_data, test_data = train_improved_classifier(df_features)\n",
        "\n",
        "    # Evaluate models\n",
        "    print(\"\\n5. Evaluating models...\")\n",
        "    models = {\n",
        "        'Logistic Regression': lr_model,\n",
        "        'Random Forest': rf_model\n",
        "    }\n",
        "\n",
        "    results = evaluate_models(models, test_data, target_names)\n",
        "\n",
        "    # Summary comparison\n",
        "    print(\"\\n=== PERFORMANCE COMPARISON ===\")\n",
        "    print(f\"{'Model':<20} {'Accuracy':<10} {'F1 Score':<10} {'Precision':<12} {'Recall':<10}\")\n",
        "    print(\"-\" * 62)\n",
        "    for name, metrics in results.items():\n",
        "        print(f\"{name:<20} {metrics['accuracy']:<10.4f} {metrics['f1']:<10.4f} \"\n",
        "              f\"{metrics['precision']:<12.4f} {metrics['recall']:<10.4f}\")\n",
        "\n",
        "    return results, word2vec_model\n",
        "\n",
        "# Run the improved model\n",
        "if __name__ == \"__main__\":\n",
        "    results, model = main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uaIhCS54sCn",
        "outputId": "8cd538ec-b0a1-4994-f92f-ce977d712b6d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== IMPROVED TEXT CLASSIFICATION MODEL ===\n",
            "\n",
            "1. Loading and preprocessing data...\n",
            "Dataset loaded with 4405 documents\n",
            "Target classes:\n",
            "0: alt.atheism\n",
            "1: comp.graphics\n",
            "2: rec.motorcycles\n",
            "3: sci.space\n",
            "4: talk.politics.guns\n",
            "After preprocessing: 4401 documents remain\n",
            "\n",
            "2. Training improved Word2Vec model...\n",
            "Training Word2Vec model...\n",
            "Word2Vec vocabulary size: 14601\n",
            "\n",
            "3. Creating document vectors...\n",
            "Document vectors created for 4401 documents\n",
            "\n",
            "4. Training classifiers...\n",
            "Training set size: 3575\n",
            "Test set size: 826\n",
            "Training Logistic Regression...\n",
            "Training Random Forest...\n",
            "\n",
            "5. Evaluating models...\n",
            "\n",
            "=== Logistic Regression Results ===\n",
            "Accuracy: 0.8838\n",
            "F1 Score: 0.8840\n",
            "Precision: 0.8844\n",
            "Recall: 0.8838\n",
            "\n",
            "Sample Predictions:\n",
            "+-----+----------+\n",
            "|label|prediction|\n",
            "+-----+----------+\n",
            "|    0|       2.0|\n",
            "|    0|       4.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       2.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "\n",
            "=== Random Forest Results ===\n",
            "Accuracy: 0.8317\n",
            "F1 Score: 0.8311\n",
            "Precision: 0.8317\n",
            "Recall: 0.8317\n",
            "\n",
            "Sample Predictions:\n",
            "+-----+----------+\n",
            "|label|prediction|\n",
            "+-----+----------+\n",
            "|    0|       2.0|\n",
            "|    0|       4.0|\n",
            "|    0|       0.0|\n",
            "|    0|       4.0|\n",
            "|    0|       2.0|\n",
            "|    0|       0.0|\n",
            "|    0|       2.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "|    0|       0.0|\n",
            "+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "\n",
            "=== PERFORMANCE COMPARISON ===\n",
            "Model                Accuracy   F1 Score   Precision    Recall    \n",
            "--------------------------------------------------------------\n",
            "Logistic Regression  0.8838     0.8840     0.8844       0.8838    \n",
            "Random Forest        0.8317     0.8311     0.8317       0.8317    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "Further improvements you can implement:\n",
        "\n",
        "1. FEATURE ENGINEERING:\n",
        "   - Use TF-IDF vectors combined with Word2Vec\n",
        "   - Add document length features\n",
        "   - Use n-gram features\n",
        "   - Implement Doc2Vec instead of averaging Word2Vec\n",
        "\n",
        "2. ADVANCED MODELS:\n",
        "   - Gradient Boosting (XGBoost)\n",
        "   - Neural Networks\n",
        "   - Ensemble methods\n",
        "\n",
        "3. HYPERPARAMETER TUNING:\n",
        "   - Use CrossValidator for parameter optimization\n",
        "   - Grid search for best parameters\n",
        "\n",
        "4. DATA AUGMENTATION:\n",
        "   - Text augmentation techniques\n",
        "   - Balancing classes with SMOTE\n",
        "\n",
        "5. ADVANCED PREPROCESSING:\n",
        "   - Lemmatization instead of just lowercasing\n",
        "   - Named Entity Recognition\n",
        "   - Part-of-speech filtering\n",
        "\n",
        "6. EVALUATION IMPROVEMENTS:\n",
        "   - Confusion matrix analysis\n",
        "   - Per-class metrics\n",
        "   - Cross-validation\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fWpQlkyX4zrw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21b8ad75"
      },
      "source": [
        "### PART 6: PREDICTING ON NEW TEXT\n",
        "\n",
        "This section adds functionality to predict the category of a new, unseen text document using the trained models."
      ]
    }
  ]
}